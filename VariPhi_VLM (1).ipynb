{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d411b7ad-dd87-47dd-8590-9216212f8c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "maestro 1.1.0rc2 requires supervision<0.26.0,>=0.20.0, but you have supervision 0.26.0rc4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"maestro[qwen_2_5_vl]==1.1.0rc2\"\n",
    "!pip install -q \"supervision==0.26.0rc4\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cc5954-40f9-4095-81f4-9c899996f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Optional\n",
    "import concurrent.futures\n",
    "import supervision as sv\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3661a7a5-f266-4941-9533-72bb2cfc1e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_ID_OR_PATH = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "MIN_PIXELS = 512 * 28 * 28\n",
    "MAX_PIXELS = 2048 * 28 * 28\n",
    "# SYSTEM_MESSAGE = None\n",
    "# PROMPT = \"\"\"\"\"\"\n",
    "SYSTEM_MESSAGE = None\n",
    "\n",
    "PROMPT =  \"Outline the position of each person and output all the coordinates in JSON format.\"\n",
    "\n",
    "NUM_WORKERS = 2 # Adjust based on your GPU memory and performance needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339a4c5c-cb1f-483c-8573-d68eb8603b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a7897dbf0b4e46ab61da370e39a384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a111593faaf41bc84220da2a209b042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da39443539684840814bf8e8a5938ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model (should be done once)\n",
    "from maestro.trainer.models.qwen_2_5_vl.checkpoints import load_model, OptimizationStrategy\n",
    "from maestro.trainer.models.qwen_2_5_vl.inference import predict_with_inputs\n",
    "from maestro.trainer.models.qwen_2_5_vl.loaders import format_conversation\n",
    "from maestro.trainer.common.utils.device import parse_device_spec\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a991a-f23d-48f0-aa84-91175e4540a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e095dd41e6c4948ba9a822423b3a801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b500b17443894f7f94cce1b7bd07a1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985e9fd81ce943b7a200a384de84546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418b216263f94779914b3c966a1db485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1806913fda884601b03732790cbb44c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556370ade34a453c94c69746056ab3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf47192f30c445fda5e2c8903494a541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0918ace6b3f84e27886bd604533be391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/57.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075e694c60ec4fedb97a0876930fbf4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c41615636e4742aaf0c398e31e7d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a16f5bc18854edc9f60df7535108280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184a8c3a391d410cbdeac21abe664c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd1fdceec9d44b8a33e0074c37e1478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee45666192704053b3a90a9d39c88353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af85953c0645418993cfdfc4058c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d7f689ca894c52bbccbb6dc5a3d37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor, model = load_model(\n",
    "    model_id_or_path=MODEL_ID_OR_PATH,\n",
    "    optimization_strategy=OptimizationStrategy.NONE,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0ae98d1-3ee8-4bba-8846-c3036eb4dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qwen_2_5_vl_inference(\n",
    "    model,\n",
    "    processor,\n",
    "    image: Image.Image,\n",
    "    prompt: str,\n",
    "    system_message: Optional[str] = None,\n",
    "    device: str = \"auto\",\n",
    "    max_new_tokens: int = 1024,\n",
    ") -> Tuple[str, Tuple[int, int]]:\n",
    "    device = parse_device_spec(device)\n",
    "    conversation = format_conversation(image=image, prefix=prompt, system_message=system_message)\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, _ = process_vision_info(conversation)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_h = inputs['image_grid_thw'][0][1] * 14\n",
    "    input_w = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "    response = predict_with_inputs(\n",
    "        **inputs,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )[0]\n",
    "\n",
    "    return response, (input_w, input_h)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc9cfed-e4df-4576-a2fe-90713bf9fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image_1.jpg: [Errno 2] No such file or directory: 'image_1.jpg'\n",
      "Error processing image_2.jpg: [Errno 2] No such file or directory: 'image_2.jpg'\n",
      "Error processing image_3.jpg: [Errno 2] No such file or directory: 'image_3.jpg'\n",
      "Error processing image_4.jpg: [Errno 2] No such file or directory: 'image_4.jpg'\n",
      "Error processing image_5.jpg: [Errno 2] No such file or directory: 'image_5.jpg'\n",
      "Error processing image_6.jpg: [Errno 2] No such file or directory: 'image_6.jpg'\n",
      "Error processing image_7.jpg: [Errno 2] No such file or directory: 'image_7.jpg'\n",
      "Error processing image_8.jpg: [Errno 2] No such file or directory: 'image_8.jpg'\n",
      "Batch processing completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATHS = [\"image_1.jpg\", \"image_2.jpg\", \"image_3.jpg\", \"image_4.jpg\" , \"image_5.jpg\",\"image_6.jpg\",\"image_7.jpg\",\"image_8.jpg\",]  \n",
    "OUTPUT_DIR = \"annotated_results\"\n",
    "SYSTEM_MESSAGE = None\n",
    "PROMPT = \"\"\"Outline the position of all the person that are working on some height and output all the coordinates in JSON format.\"\"\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def process_and_save_images(image_paths: List[str]):\n",
    "    \"\"\"Process multiple images and save annotated results\"\"\"\n",
    "    box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path)\n",
    "            resolution_wh = image.size\n",
    "            \n",
    "            # Run inference\n",
    "            response, input_wh = run_qwen_2_5_vl_inference(\n",
    "                model=model,\n",
    "                processor=processor,\n",
    "                image=image,\n",
    "                prompt=PROMPT,\n",
    "                system_message=SYSTEM_MESSAGE\n",
    "            )\n",
    "            \n",
    "            print(f\"Results for {image_path}:\")\n",
    "            print(response)\n",
    "\n",
    "            # Parse detections\n",
    "            detections = sv.Detections.from_vlm(\n",
    "                vlm=sv.VLM.QWEN_2_5_VL,\n",
    "                result=response,\n",
    "                input_wh=input_wh,\n",
    "                resolution_wh=resolution_wh\n",
    "            )\n",
    "\n",
    "            # Annotate image\n",
    "            annotated_image = np.array(image.copy())\n",
    "            annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "            annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "            \n",
    "            # Convert back to PIL Image and save\n",
    "            output_path = os.path.join(OUTPUT_DIR, f\"annotated_{os.path.basename(image_path)}\")\n",
    "            Image.fromarray(annotated_image).convert('RGB').save(output_path)\n",
    "            print(f\"Saved annotated image to {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_save_images(IMAGE_PATHS)\n",
    "    print(\"Batch processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547518a-1e9f-4f1b-9f8a-4b5c5ecd5948",
   "metadata": {},
   "source": [
    "USING THREADING OPERATION !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6946eadf-0d48-45e5-bd26-520278833cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6288d987-877b-479c-9d68-f441569f3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(image_path: str):\n",
    "    \"\"\"Process and annotate a single image.\"\"\"\n",
    "    box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "    try:\n",
    "        PROMPT = \"\"\"Outline the position of all the person that are working on some height and output all the coordinates in STRICTLY in JSON format.\"\"\"\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        resolution_wh = image.size\n",
    "\n",
    "        # Run inference\n",
    "        response, input_wh = run_qwen_2_5_vl_inference(\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            image=image,\n",
    "            prompt=PROMPT,\n",
    "            system_message=SYSTEM_MESSAGE\n",
    "        )\n",
    "\n",
    "        print(f\"Results for {image_path}:\")\n",
    "        print(response)\n",
    "\n",
    "        # Parse detections\n",
    "        detections = sv.Detections.from_vlm(\n",
    "            vlm=sv.VLM.QWEN_2_5_VL,\n",
    "            result=response,\n",
    "            input_wh=input_wh,\n",
    "            resolution_wh=resolution_wh\n",
    "        )\n",
    "\n",
    "        # Annotate image\n",
    "        annotated_image = np.array(image.copy())\n",
    "        annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "        annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "        # Save result\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"annotated_{os.path.basename(image_path)}\")\n",
    "        Image.fromarray(annotated_image).convert('RGB').save(output_path)\n",
    "        print(f\"Saved annotated image to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "616cdde5-06b1-47be-99b3-fed2323e4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_images(image_paths: List[str], max_workers: int = 1):\n",
    "    \"\"\"Parallel image processing using threads.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_image, path) for path in image_paths]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # to raise any exceptions if occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3ddb252-3f5f-40cd-80c3-4745dd26fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATHS = [\"image_1.jpg\", \"image_2.jpg\", \"image_3.jpg\", \"image_4.jpg\" , \"image_5.jpg\",\"image_6.jpg\",\"image_7.jpg\",\"image_8.jpg\",]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8c19069-2f0e-4853-b5df-07555dad36d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for image_1.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [153, 109, 286, 334], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [197, 73, 357, 242], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [481, 95, 551, 245], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [626, 116, 677, 269], \"label\": \"person that are working on some height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_image_1.jpg\n",
      "Results for image_2.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [395, 0, 471, 151], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [480, 108, 530, 246], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [414, 263, 468, 408], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [462, 390, 524, 550], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [530, 452, 582, 550], \"label\": \"person that are working on some height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_image_2.jpg\n",
      "Results for image_4.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [179, 154, 430, 531], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [426, 201, 730, 508], \"label\": \"person that are working on some height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_image_4.jpg\n",
      "Results for image_5.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [138, 197, 264, 315], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [366, 320, 474, 435], \"label\": \"person that are working on some height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_image_5.jpg\n",
      "Error processing image_6.jpg: [Errno 2] No such file or directory: 'image_6.jpg'\n",
      "Results for image_7.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [87, 204, 139, 356], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [174, 88, 224, 233], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [219, 323, 271, 444], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [270, 327, 333, 496], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [323, 264, 380, 416], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [377, 136, 429, 208], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [617, 109, 690, 252], \"label\": \"person that are working on some height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_image_7.jpg\n",
      "Results for image_8.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [146, 309, 237, 428], \"label\": \"person that are working on some height\"},\n",
      "\t{\"bbox_2d\": [225, 107, 297, 174], \"label\": \"person that are working on some height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_image_8.jpg\n",
      "Batch processing completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_and_save_images(IMAGE_PATHS, max_workers=1)  # 2 threads: each handles half\n",
    "    print(\"Batch processing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1eafbcc-1d36-4409-ad85-cad9100fd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(image_path: str):\n",
    "    \"\"\"Process and annotate a single image.\"\"\"\n",
    "    box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "    try:\n",
    "        PROMPT = \"\"\"Detect and outline the position of all persons working at height. Additionally, identify if any worker is (1) smoking a cigarette or (2) using a mobile phone. Output all relevant coordinates and detected actions STRICTLY in JSON format.\"\"\"\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        resolution_wh = image.size\n",
    "\n",
    "        # Run inference\n",
    "        response, input_wh = run_qwen_2_5_vl_inference(\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            image=image,\n",
    "            prompt=PROMPT,\n",
    "            system_message=SYSTEM_MESSAGE\n",
    "        )\n",
    "\n",
    "        print(f\"Results for {image_path}:\")\n",
    "        print(response)\n",
    "\n",
    "        # Parse detections\n",
    "        detections = sv.Detections.from_vlm(\n",
    "            vlm=sv.VLM.QWEN_2_5_VL,\n",
    "            result=response,\n",
    "            input_wh=input_wh,\n",
    "            resolution_wh=resolution_wh\n",
    "        )\n",
    "\n",
    "        # Annotate image\n",
    "        annotated_image = np.array(image.copy())\n",
    "        annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "        annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "\n",
    "        # Save result\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"annotated_{os.path.basename(image_path)}\")\n",
    "        Image.fromarray(annotated_image).convert('RGB').save(output_path)\n",
    "        print(f\"Saved annotated image to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64c6d22-348b-4c10-974d-e8e6839affff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_images(image_paths: List[str], max_workers: int = 1):\n",
    "    \"\"\"Parallel image processing using threads.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_image, path) for path in image_paths]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # to raise any exceptions if occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7f41ec-0814-47d7-8b72-6d5f4d165ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATHS = [\n",
    "    \"imagee_1.jpg\", \"imagee_2.jpg\", \"imagee_3.jpg\", \"imagee_4.jpg\", \"imagee_5.jpg\",\n",
    "    \"imagee_6.jpg\", \"imagee_7.jpg\", \"imagee_8.jpg\", \"imagee_9.jpg\", \"imagee_10.jpg\",\n",
    "    \"imagee_11.jpg\", \"imagee_12.jpg\", \"imagee_13.jpg\", \"imagee_14.jpg\", \"imagee_15.jpg\",\n",
    "    \"imagee_16.jpg\", \"imagee_17.jpg\", \"imagee_18.jpg\", \"imagee_19.jpg\", \"imagee_20.jpg\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3362ca7-537e-4b76-b6bc-220bb5f28a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for imagee_1.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [283, 397, 635, 1007], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [370, 494, 486, 612], \"label\": \"(2) using a mobile phone\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_1.jpg\n",
      "Results for imagee_2.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [500, 34, 716, 800], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [872, 630, 907, 719], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [919, 681, 945, 720], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [500, 34, 716, 800], \"label\": \"(2) using a mobile phone\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_2.jpg\n",
      "Results for imagee_3.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [30, 0, 394, 415], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [486, 87, 757, 415], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [198, 76, 230, 129], \"label\": \"(2) using a mobile phone\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_3.jpg\n",
      "Results for imagee_4.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [300, 31, 625, 498], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [300, 31, 625, 498], \"label\": \"(1) smoking a cigarette\"},\n",
      "\t{\"bbox_2d\": [300, 31, 625, 498], \"label\": \"(2) using a mobile phone\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_4.jpg\n",
      "Results for imagee_5.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [609, 58, 983, 672], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [61, 64, 272, 672], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [399, 58, 443, 180], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [438, 94, 493, 165], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [482, 36, 541, 181], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [574, 38, 627, 157], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [627, 38, 672, 157], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [609, 58, 983, 672], \"label\": \"smoking a cigarette\"},\n",
      "\t{\"bbox_2d\": [61, 64, 272, 672], \"label\": \"using a mobile phone\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_5.jpg\n",
      "Results for imagee_6.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [0, 138, 165, 531], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [174, 114, 360, 531], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [386, 130, 559, 531], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [579, 167, 784, 531], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_6.jpg\n",
      "Results for imagee_7.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [53, 58, 390, 532], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [53, 58, 390, 532], \"label\": \"(1) smoking a cigarette\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_7.jpg\n",
      "Results for imagee_8.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [176, 65, 398, 499], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [176, 65, 398, 499], \"label\": \"(1) smoking a cigarette\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_8.jpg\n",
      "Results for imagee_9.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [348, 70, 756, 532], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [29, 16, 616, 532], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [29, 16, 616, 532], \"label\": \"(1) smoking a cigarette\"},\n",
      "\t{\"bbox_2d\": [348, 70, 756, 532], \"label\": \"(2) using a mobile phone\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_9.jpg\n",
      "Results for imagee_10.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [309, 78, 605, 532], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [309, 78, 605, 532], \"label\": \"(2) using a mobile phone\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_10.jpg\n",
      "Results for imagee_11.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [503, 149, 716, 518], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [312, 136, 504, 509], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [119, 150, 322, 510], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [119, 150, 322, 510], \"label\": \"(1) smoking a cigarette\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_11.jpg\n",
      "Error processing imagee_12.jpg: [Errno 2] No such file or directory: 'imagee_12.jpg'\n",
      "Results for imagee_13.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [840, 15, 873, 62], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [446, 342, 571, 590], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_13.jpg\n",
      "Results for imagee_14.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [450, 203, 730, 486], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [179, 155, 428, 531], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [450, 203, 730, 486], \"label\": \"(1) smoking a cigarette\"},\n",
      "\t{\"bbox_2d\": [179, 155, 428, 531], \"label\": \"(1) smoking a cigarette\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_14.jpg\n",
      "Results for imagee_15.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [365, 324, 470, 438], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [139, 199, 262, 313], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [409, 18, 487, 137], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [465, 108, 550, 227], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_15.jpg\n",
      "Results for imagee_16.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [538, 286, 579, 374], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [350, 313, 398, 374], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [318, 405, 363, 456], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [132, 397, 176, 450], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [143, 556, 206, 625], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [400, 509, 450, 572], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [558, 524, 625, 635], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_16.jpg\n",
      "Results for imagee_17.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [89, 204, 137, 356], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [172, 88, 224, 233], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [278, 165, 339, 282], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [322, 265, 382, 415], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [377, 136, 428, 209], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [618, 109, 690, 252], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [194, 314, 253, 445], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [266, 327, 332, 496], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_17.jpg\n",
      "Results for imagee_18.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [305, 234, 418, 417], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [480, 256, 553, 442], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [149, 322, 235, 427], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [358, 164, 410, 227], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [356, 31, 402, 147], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [398, 26, 441, 132], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [228, 105, 297, 168], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [23, 81, 82, 148], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_18.jpg\n",
      "Results for imagee_19.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [145, 298, 216, 403], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [267, 295, 330, 432], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [269, 209, 320, 295], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [192, 197, 238, 295], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [376, 162, 425, 250], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [477, 135, 547, 290], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [538, 135, 612, 291], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_19.jpg\n",
      "Results for imagee_20.jpg:\n",
      "```json\n",
      "[\n",
      "\t{\"bbox_2d\": [98, 193, 184, 367], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [250, 185, 355, 367], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [484, 202, 599, 376], \"label\": \"persons working at height\"},\n",
      "\t{\"bbox_2d\": [579, 181, 735, 370], \"label\": \"persons working at height\"}\n",
      "]\n",
      "```\n",
      "Saved annotated image to annotated_results/annotated_imagee_20.jpg\n",
      "Batch processing completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_and_save_images(IMAGE_PATHS, max_workers=1)  # 2 threads: each handles half\n",
    "    print(\"Batch processing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d2417-a2bf-49ed-88f3-2b68c175c8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
